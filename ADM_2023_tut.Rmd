---
title: "R Notebook"
output: github_document
---

#partie 1
```{bash, eval=FALSE}
wget https://github.com/ANF-MetaBioDiv/course-material/archive/refs/heads/main.zip
unzip main.zip
# télécharger le repertoire des cours qui contient les data a utiliser 
```

```{r}
refdb_folder <- here::here("course-material-main", "data", "refdb")
refdb_folder
#sauvegarder la variable comme le chemin où on met nos data de ref
```

```{r, eval=FALSE}
if (!dir.exists(refdb_folder)) dir.create(refdb_folder, recursive =TRUE) #créer le vrai dossier rfdb folder, mettre la commande de "s'il n'existe pas alors créer le" pour ne pas le supprimer à chaque fois

#ctrl alt i -> créer un nv chunk
```

```{bash, eval=FALSE}
#mettre la direction où on veut aller avant
cp -R course-material-main/data/raw ./data
#copier les séqeunces d'ADN Reads dans le dossier data qui est dans la direction qu'on a mise avant
```

```{r}
# mettre le temps de chargement à 60 sec
getOption("timeout")
```

```{r}
# on change le temps de chargement à 60sec
options(timeout = 60)

# télécharger les bases de données dada2 formatées depuis Silva
silva_train_set <- file.path(refdb_folder,
                             "silva_nr99_v138.1_train_set.fa.gz")

silva_species_assignment <- file.path(refdb_folder,
                                      "silva_species_assignment_v138.1.fa.gz")
```

```{r}
# then we download the files if they don't already exist

if (!file.exists(silva_train_set)) {
  download.file(
    "https://zenodo.org/record/4587955/files/silva_nr99_v138.1_train_set.fa.gz",
    silva_train_set,
    quiet = TRUE
  )
}

if (!file.exists(silva_species_assignment)) {
  download.file(
    "https://zenodo.org/record/4587955/files/silva_species_assignment_v138.1.fa.gz",
    silva_species_assignment,
    quiet = TRUE
  )
}
```

#partie 2
```{r}
path_to_fastqs <- here::here("data", "raw")
# définir le chemin pour aller aux Fastas qui s'adaptera à chaque ordinateur
```

```{r}
fnFs <- sort(list.files(path_to_fastqs,
                        pattern = "_R1.fastq.gz",
                        full.names = TRUE)) 
# définir nvlle variable qui contient tous les fichiers des fasta

# seulement sélectionner les noms de fichiers qui se finissent par ce qu'on met en pattern
```

```{r}
fnRs <- sort(list.files(path_to_fastqs,
                        pattern = "_R2.fastq.gz",
                        full.names = TRUE))
# same pour les séquences inverses
```


```{r}
sample_names <- basename(fnFs) |>
  strsplit(split = "_") |> # diviser la chaîne de carac selon le modèle mis entre ""
  sapply(head, 1) # appliquer une fonction à chaque éléments d'une liste
sample_names
```

```{r}
basename(fnFs) |>
  head() #lister les fichiers qui ont R1
```
```{r}
basename(fnFs) |>
  strsplit(split = "_") |> # séparer les noms des fichiers en deux vecteurs
  head()
```

```{r}
gsub("^.+/|_.+$", "", fnFs) |> head()
```

```{r}
devtools::load_all(path="/home/rstudio/ADM_2023_tut/course-material-main/R")
# pas à l'endroit voulu car sinon ne fonctionne pas
# utiliser devtools à la place de source(), sert à télécharger les fonctions spé pour ce tutoriel
```

#partie 3

```{r}
# on crée une direction pour les sorties
quality_folder <- here::here("outputs",
                             "dada2",
                             "quality_plots")

if (!dir.exists(quality_folder)) {
  dir.create(quality_folder, recursive = TRUE)
}

qualityprofile(fnFs,
               fnRs,
               file.path(quality_folder, "quality_plots.pdf"))
```
#partie 4
```{r}
#on crée un dossier où sauvegarder les Reads après qu'elles aient été élaguées
path_to_trimmed_reads <- here::here(
  "outputs",
  "dada2",
  "trimmed"
)

if (!dir.exists(path_to_trimmed_reads)) dir.create(path_to_trimmed_reads, recursive = TRUE)
```

```{r}
#on crée les primers
primer_fwd  <- "CCTACGGGNBGCASCAG"
primer_rev  <- "GACTACNVGGGTATCTAAT"
```

```{r}
#trouver les amorces F dans les reads 1
Biostrings::readDNAStringSet(
  fnFs[1],
  format = "fastq",
  nrec = 10
)
```

```{r}
#trouver les amorces R dans les reads 2
Biostrings::readDNAStringSet(
  fnRs[1],
  format = "fastq",
  nrec = 10
)
```

```{bash}
pwd #copier le bash dans mon dossier de travail (dada2)
cp -R /home/rstudio/ADM_2023_tut/course-material-main/bash .
```

```{r}
#on enlève les séquences des amorces F et R dans les reads 1 et 2
(primer_log <- primer_trim(
  forward_files = fnFs,
  reverse_files = fnRs,
  primer_fwd = primer_fwd,
  primer_rev = primer_rev,
  output_dir = path_to_trimmed_reads,
  min_size = 200
))
```

```{r}
nopFw <- sort(list.files(path_to_trimmed_reads, pattern = "R1", full.names = TRUE))
nopRv <- sort(list.files(path_to_trimmed_reads, pattern = "R2", full.names = TRUE))
print(nopRv)
print(nopFw)
```

#partie 5

```{r}
#créer un fichier
path_to_filtered_reads <- here::here("outputs", "dada2", "filtered")
if (!dir.exists(path_to_filtered_reads)) dir.create(path_to_filtered_reads, recursive = TRUE)
```


```{r}
#lister les chemins
filtFs <- file.path(path_to_filtered_reads, basename(fnFs))
filtRs <- file.path(path_to_filtered_reads, basename(fnRs))
```


```{r}
#faire les liens entre les fichiers et les noms des échantillons
names(filtFs) <- sample_names
names(filtRs) <- sample_names
```

```{r}

(out <- dada2::filterAndTrim(
  fwd = nopFw, #entrée où sont les reads forward sans les primers
  filt = filtFs, # sorties où sont les reads forward filtrée
  rev = nopRv,
  filt.rev = filtRs, #la même chose mais pour les reverse
  minLen = 150,
  matchIDs = TRUE,
  maxN = 0, #la valeur max acceptée de base pas sûres
  maxEE = c(3, 3), #read expected errors (EE) seuil, c'est la somme des probabilités d'erreur de chaque base qui compose le read. augmenter cette valeur permet d'accepter plus de reads de basse qualité. La première valeur c'est les forward reads, et la deuxième c'est les reverse
  truncQ = 2 #tronquer les reads à la première vue d'un score de qualité <= à truncQ
))
```
#partie 6

```{r}
#pour enlever le bruit de fond de notre data, on doit utiliser une erreur modèle, on peut l'apprendre directement par la fonction qui suit 
errF <- dada2::learnErrors(filtFs,
                           randomize = TRUE,
                           multithread = TRUE)
```

```{r}
errR <- dada2::learnErrors(filtRs,
                           randomize = TRUE,
                           multithread = TRUE)
```

```{r}
#le résultat du modèle d'erreur
dada2::plotErrors(errF, nominalQ=TRUE)
```

```{r}
#avant d'enlever le bruit de fond, on doit dupliquer les séquences, pour chaque séquence il faut compter le nbr de reads
derepFs <- dada2::derepFastq(filtFs, verbose = TRUE)

derepRs <- dada2::derepFastq(filtRs, verbose = TRUE)
```

```{r}
# et là on run la fonction pour enlever le bruit de fond
dadaFs <- dada2::dada(derepFs, err = errF, multithread = TRUE)
```

```{r}
#same pour les reverses
dadaRs <- dada2::dada(derepRs, err = errR, multithread = TRUE)
```

#partie 7

```{r}
#on peut les fusionner
mergers <- dada2::mergePairs(
  dadaF = dadaFs,
  derepF = derepFs,
  dadaR = dadaRs,
  derepR = derepRs,
  maxMismatch = 0,
  verbose = TRUE
)
```

#partie 8

```{r}
# à ce stade on a les ASV et on sait le nombre de reads de chaque échantillon, donc on peut faire une ASV table (table de comptage)
seqtab <- dada2::makeSequenceTable(mergers)
```

#partie 9

```{r}
#les chimères sont les séquences artefacts formées par 2 ou + séquences biologiques qui sont mal jointes
# on trouve et supprime les bimères (deux chimères parents) :
seqtab_nochim <- dada2::removeBimeraDenovo(seqtab,
                                           method = "consensus",
                                           multithread = TRUE,
                                           verbose = TRUE)
```

#partie 10

```{r}
#la table ASV est prête, mais sans avoir aucune info sur l'identité taxonomique, donc on n'irait pas très loin dans notre interprétation écologique
# on peut avoir une idée de l'identité taxonomique ASV en coparant leur séquence à une database de référence comme SILVA, on le fait en deux étapes
# 1 : chaque ASV est assignée à une taxonomie en utilisant le RDP Naive Bayesian Classifier algorithme décrit dans un papier appelé par la fonction d'après 

taxonomy <- dada2::assignTaxonomy(
  seqs = seqtab_nochim,
  refFasta = silva_train_set,
  taxLevels = c("Kingdom", "Phylum", "Class",
                "Order", "Family", "Genus",
                "Species"),
  multithread = TRUE,
  minBoot = 60
)
```

```{r}
#la méthode est robuste mais ça loupe pour assigner à l'espèce
#si on considère qu'une ASV est 100% similaireà une séq de ref, ça appartient à la même sp, donc on peut utiliser cette fonction :

taxonomy <- dada2::addSpecies(
  taxonomy,
  silva_species_assignment,
  allowMultiple = FALSE
)   
```

#partie 11

```{r}
#et là on exporte tout, comme des objets R, un pour l'ASV et l'autre pour la taxonomy

export_folder <- here::here("outputs", "dada2", "asv_table")

if (!dir.exists(export_folder)) dir.create(export_folder, recursive = TRUE)

saveRDS(object = seqtab_nochim,
        file = file.path(export_folder, "seqtab_nochim.rds"))

saveRDS(object = taxonomy,
        file = file.path(export_folder, "taxonomy.rds"))
```


```{r}
# exporter as text, pour qu'il soit réutilisanle dans d'autres programmes ou language de programmations, mais avant faut formater la data un peu
#d'abord on crée une nouvelle variable pour collecter les séquences ASV
asv_seq <- colnames(seqtab_nochim)
```

```{r}
#on crée des identités uniques pour chaque ASV, la séquence elle-même est une id unique, mais on veut un truc plus court
ndigits <- nchar(length(asv_seq))
asv_id <- sprintf(paste0("ASV_%0", ndigits, "d"), seq_along(asv_seq))
```


```{r}
# on renomme tout avec les nouvelles variables
row.names(taxonomy) <- colnames(seqtab_nochim) <- names(asv_seq) <- asv_id
```

```{r}
#on converti les row names (les ASV id) dans une nouvelle colonne nommée asv
taxonomy_export <- df_export(taxonomy, new_rn = "asv")

seqtab_nochim_export <- t(seqtab_nochim)
seqtab_nochim_export <- df_export(seqtab_nochim_export, new_rn = "asv")
```

```{r}
#on exporte la taxonomie
write.table(taxonomy_export,
            file = file.path(export_folder, "taxonomy.tsv"),
            quote = FALSE,
            sep = "\t",
            row.names = FALSE)
```

```{r}
#on exporte la table ASV
write.table(seqtab_nochim_export,
            file = file.path(export_folder, "asv_table.tsv"),
            quote = FALSE,
            sep = "\t",
            row.names = FALSE)
```

```{r}
#et les séquences comme étant des fichier fasta
cat(paste0(">", names(asv_seq), "\n", asv_seq),
    sep = "\n",
    file = file.path(export_folder, "asv.fasta"))
```

```{r}
#on assemble le tableau
getN <- function(x) sum(dada2::getUniques(x))

log_table <- data.frame(
  input = primer_log$in_reads,
  with_fwd_primer = primer_log$`w/adapters`,
  with_rev_primer = primer_log$`w/adapters2` ,
  with_both_primers = out[, 1],
  filtered = out[, 2],
  denoisedF = sapply(dadaFs, getN),
  denoisedR = sapply(dadaRs, getN),
  merged = sapply(mergers, getN),
  nonchim = rowSums(seqtab_nochim),
  perc_retained = rowSums(seqtab_nochim) / out[, 1] * 100
)

rownames(log_table) <- sample_names
```

```{r}
#et on l'exporte
df_export(log_table, new_rn = "sample") |>
  write.table(file = file.path(export_folder, "log_table.tsv"),
              quote = FALSE,
              sep = "\t",
              row.names = FALSE)
```
